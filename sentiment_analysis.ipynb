{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################   Part 1 : Data Preprocessing   ############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data\n",
    "file_name = \"reviews.csv\"\n",
    "reviews = pd.read_csv(file_name)\n",
    "reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the score\n",
    "sns.distplot(a=reviews[\"Score\"], kde=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing rows with score of 3\n",
    "filtered_reviews = reviews.loc[reviews[\"Score\"].isin([1, 2, 4, 5])]\n",
    "\n",
    "def category(x):\n",
    "    if x > 3:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "ratings = filtered_reviews[\"Score\"].map(category)\n",
    "filtered_reviews[\"Score\"] = ratings\n",
    "\n",
    "filtered_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data cleaning\n",
    "sorted_reviews = filtered_reviews.sort_values(by=\"ProductId\", axis=0, ascending=True)\n",
    "final_reviews = sorted_reviews.drop_duplicates(subset={\"UserId\", \"ProfileName\", \"Time\", \"Text\"}, keep=\"first\", inplace=False)\n",
    "final_reviews.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# % of data left\n",
    "(final_reviews.size*1.0 / filtered_reviews.size*1.0) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing incorrect entries\n",
    "final_reviews = final_reviews[final_reviews.HelpfulnessNumerator <= final_reviews.HelpfulnessDenominator]\n",
    "final_reviews.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Value counts\n",
    "final_reviews[\"Score\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a list of stopwords\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "stop_words = stopwords.words(\"english\")\n",
    "\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text preprocessing\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import contractions\n",
    "preprocessed_reviews = []\n",
    "\n",
    "for sentence in tqdm(final_reviews['Text'].values):\n",
    "    sentence = re.sub(r\"http\\S+\", \"\", sentence)\n",
    "    sentence = BeautifulSoup(sentence, 'lxml').get_text()\n",
    "    sentence = contractions.fix(sentence)\n",
    "    sentence = re.sub(\"\\S*\\d\\S*\", \"\", sentence).strip()\n",
    "    sentence = re.sub('[^A-Za-z]+', ' ', sentence)\n",
    "    sentence = sentence.lower()\n",
    "    sentence = stemmer.stem(sentence)\n",
    "    sentence = \" \".join([word for word in sentence.split() if word not in stop_words])\n",
    "    preprocessed_reviews.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the cleaned text into a csv\n",
    "final_reviews[\"Cleaned Text\"] = preprocessed_reviews\n",
    "final_reviews.to_csv(path_or_buf=\"cleaned_reviews.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating vocubulary\n",
    "corpus = []\n",
    "\n",
    "for text in tqdm(final_reviews[\"Cleaned Text\"]):\n",
    "    for word in text.strip().split():\n",
    "        corpus.append(word.strip())\n",
    "    \n",
    "print(len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word count\n",
    "from collections import Counter\n",
    "word_count = Counter(corpus)\n",
    "print(\"Unique words =\", len(word_count))\n",
    "word_count.most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating word count dataframe\n",
    "word_count_df = []\n",
    "\n",
    "for idx, (word, count) in enumerate(word_count.most_common(len(word_count))):\n",
    "    word_count_df.append([idx+1, word, count])\n",
    "\n",
    "word_count_df = pd.DataFrame(columns=[\"Index\", \"Word\", \"Count\"], data=word_count_df)    \n",
    "word_count_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating word count dictionary\n",
    "word_count_dict = {}\n",
    "\n",
    "for _, row in word_count_df.iterrows():\n",
    "    word_count_dict[row[\"Word\"]] = [row[\"Index\"], row[\"Count\"]]\n",
    "\n",
    "print(word_count_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Data preprocessing\n",
    "indexed_X = []\n",
    "indexed_y = []\n",
    "\n",
    "for sentence in final_reviews[\"Cleaned Text\"]:\n",
    "    indexed_X.append([word_count_dict[word][0] for word in sentence.strip().split()])\n",
    "\n",
    "indexed_y = final_reviews[\"Score\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def load_data(num_words):\n",
    "    X = indexed_X\n",
    "    y = indexed_y\n",
    "    \n",
    "    for i in range(len(X)):\n",
    "        for j in range(len(X[i])):\n",
    "            if X[i][j] > num_words:\n",
    "                X[i][j] = 0\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n",
    "    return (X_train, y_train), (X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################   Part 2 : Model Building   ############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words = 5000\n",
    "(X_train, y_train), (X_test, y_test) = load_data(num_words=top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Truncate and/or pad input sequences\n",
    "max_review_length = 600\n",
    "\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "embedding_vecor_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words+1, embedding_vecor_length, input_length=max_review_length))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################   Part 3 : Model Evaluation   ############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the model\n",
    "model.fit(X_train, y_train, nb_epoch=10, batch_size=64)\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
